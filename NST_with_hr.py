# -*- coding: utf-8 -*-
"""NST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M0DpDcox4Y4bAjJQLSyGtlE7eQ6CwCA9
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
from torchvision import models,transforms
import torch.optim as optim
from PIL import Image
from io import BytesIO
import matplotlib.pyplot as plt
import numpy as np

"""LOADING MODEL"""

vgg=models.vgg19(pretrained=True).features

for parameter in vgg.parameters():
    parameter.requires_grad_(False)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

vgg.to(device)

"""IMAGE LAODER AND TRANSFROMS"""

from google.colab import drive
drive.mount('/content/drive')

style_img_path="/content/drive/MyDrive/Images/HistogramMatch.jpeg"
content_img_path="/content/drive/MyDrive/Images/Tuebingen_Neckarfront.jpg"

content_img=Image.open(content_img_path).convert('RGB')
style_img=Image.open(style_img_path).convert('RGB')

size=512

def imageLoader(img,size):
  img_transform = transforms.Compose([
                        transforms.Resize(size),
                        transforms.ToTensor(),
                        transforms.Normalize((0.485, 0.456, 0.406),
                                             (0.229, 0.224, 0.225))])

  img=img_transform(img).unsqueeze(0)

  return img


def imageUnLoader(tensor):
    image = tensor.to("cpu").clone().detach()
    image = image.numpy().squeeze()

    image = image.transpose(1,2,0)
    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))
    image = image.clip(0, 1)
    return image

imgs_tensor=[imageLoader(content_img,size).to(device),imageLoader(style_img,size).to(device)]
content,style=imgs_tensor

# display the images
fig, (ax1, ax2) = plt.subplots(1, 2)
# content and style ims side-by-side
ax1.imshow(imageUnLoader(content))
ax2.imshow(imageUnLoader(style))

for name, layer in vgg._modules.items():
  print(name)
  print(layer)

def featureMapExtractor(image,model,layers):
  if layers=="style":
    layers=["3", "8", "17", "26", "35"]

  if layers=="content":
    layers= ["22"]

  if layers=="generated":
    layers=["22","3", "8", "17", "26", "35"]


  features={}
  x=image
  for name, layer in model._modules.items():
    if isinstance(layer, nn.ReLU):
        x = F.relu(x, inplace=False)
    elif isinstance(layer, nn.MaxPool2d):
        x = nn.MaxPool2d(2, 2)(x)
    else:
        x = layer(x)

    if name in layers:
        features[name] = x
  return features

"""Defining GramMatrix"""

def gramMatrix(tensor):
  b,d,h,w=tensor.size()

  tensor=tensor.view(b*d,h*w)

  gram = torch.mm(tensor, tensor.t())

  gram = gram.div(b * d * h * w)

  return gram

content_fms=featureMapExtractor(content,vgg,"content")
style_fms=featureMapExtractor(style,vgg,"style")
style_gram={layer: gramMatrix(style_fms[layer]) for layer in style_fms}
generated_img=content.clone().requires_grad_(True).to(device)

style_weights = [1e12 / n**2 for n in [64, 128, 256, 512, 512]]
alpha = 3e0
beta=2e9
style_layers=["3", "8", "17", "26", "35"]

"""Style Transfer"""

num_of_steps=5000
show_iter=500
step=0

optimizer = optim.Adam(params=[generated_img], lr=0.003)  # Adjust learning rate as needed

while step <= num_of_steps:

    generated_img_features = featureMapExtractor(generated_img, vgg, "generated")

    content_loss = torch.mean((generated_img_features["22"] - content_fms["22"])**2)

    style_loss = 0
    i = 0
    for layer in style_layers:
        generated_img_feature = generated_img_features[layer]
        generated_img_gram = gramMatrix(generated_img_feature)
        _, d, h, w = generated_img_feature.shape
        style_img_gram = style_gram[layer]
        layer_style_loss = style_weights[i] * torch.mean((generated_img_gram - style_img_gram)**2)
        style_loss += layer_style_loss / (d * h * w)
        i += 1

    total_loss = alpha * content_loss + beta * style_loss

    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()

    if step % show_iter == 0:
        print('Step {}: Total loss: {:.4f}'.format(step, total_loss.item()))
        plt.imshow(imageUnLoader(generated_img))
        plt.show()

    step += 1

generated_out_img=imageUnLoader(generated_img)
plt.imshow(generated_out_img)
plt.show()

# Ensure the data type is uint8
generated_out_img = (generated_out_img * 255).astype(np.uint8)

# Create the PIL Image object
generated_out_img_pil = Image.fromarray(generated_out_img)

# Save the image as a file
generated_out_img_pil.save("generated_image.png")

"""For High Res Transfer"""

image_hr=1024

imgs_tensor=[imageLoader(generated_out_img_pil,image_hr).to(device),imageLoader(style_img,image_hr).to(device)]
generated_output_img,style2=imgs_tensor

generated_fms=featureMapExtractor(generated_output_img,vgg,"content")
style_fms=featureMapExtractor(style2,vgg,"style")
style_gram={layer: gramMatrix(style_fms[layer]) for layer in style_fms}
generated_img_hr=generated_output_img.clone().requires_grad_(True).to(device)

style_weights = [1e12 / n**2 for n in [64, 128, 256, 512, 512]]
alpha = 1e0
beta=1e9
style_layers=["3", "8", "17", "26", "35"]

num_of_steps=200
show_iter=50
step=0

optimizer = optim.Adam(params=[generated_img_hr], lr=0.003)  # Adjust learning rate as needed

while step <= num_of_steps:

    generated_img_hr_features = featureMapExtractor(generated_img_hr, vgg, "generated")

    generated_loss = torch.mean((generated_img_hr_features["22"] - generated_fms["22"])**2)

    style_loss = 0
    i = 0
    for layer in style_layers:
        generated_img_hr_feature = generated_img_hr_features[layer]
        generated_img_hr_gram = gramMatrix(generated_img_hr_feature)
        _, d, h, w = generated_img_hr_feature.shape
        style_img_gram = style_gram[layer]
        layer_style_loss = style_weights[i] * torch.mean((generated_img_hr_gram - style_img_gram)**2)
        style_loss += layer_style_loss / (d * h * w)
        i += 1

    total_loss = alpha * generated_loss + beta * style_loss

    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()

    if step % show_iter == 0:
        print('Step {}: Total loss: {:.4f}'.format(step, total_loss.item()))
        plt.imshow(imageUnLoader(generated_img_hr))
        plt.show()

    step += 1

generated_out_img_hr=imageUnLoader(generated_img_hr)
plt.imshow(generated_out_img_hr)
plt.show()

# Ensure the data type is uint8
generated_out_img_hr = (generated_out_img_hr * 255).astype(np.uint8)

# Create the PIL Image object
generated_out_img_hr_pil = Image.fromarray(generated_out_img_hr)

# Save the image as a file
generated_out_img_hr_pil.save("generated_image_hr.png")

